"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[246],{3836:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>g,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"getting-started/recipes/chatml","title":"Counting Tokens for ChatML","description":"If you are using the OpenAI chat models, you need to account for additional tokens that are added to the input text. This recipe shows how to do that. It is based on this OpenAI Cookbook example.","source":"@site/docs/getting-started/recipes/chatml.md","sourceDirName":"getting-started/recipes","slug":"/getting-started/recipes/chatml","permalink":"/docs/getting-started/recipes/chatml","draft":false,"unlisted":false,"editUrl":"https://github.com/knuddelsgmbh/jtokkit/tree/main/docs/docs/getting-started/recipes/chatml.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Counting Tokens for ChatML","sidebar_position":1},"sidebar":"gettingStarted","previous":{"title":"Extending JTokkit","permalink":"/docs/getting-started/extending"}}');var o=n(4848),i=n(8453);const a={title:"Counting Tokens for ChatML",sidebar_position:1},r=void 0,c={},d=[];function m(t){const e={a:"a",code:"code",p:"p",pre:"pre",...(0,i.R)(),...t.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:["If you are using the OpenAI chat models, you need to account for additional tokens that are added to the input text. This recipe shows how to do that. It is based on this ",(0,o.jsx)(e.a,{href:"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb",children:"OpenAI Cookbook example"}),"."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-java",children:'private int countMessageTokens(\n\t\tEncodingRegistry registry,\n\t\tString model,\n\t\tList<ChatMessage> messages // consists of role, content and an optional name\n) {\n\tEncoding encoding = registry.getEncodingForModel(model).orElseThrow();\n\tint tokensPerMessage;\n\tint tokensPerName\n\tif (model.startsWith("gpt-4")) {\n\t\ttokensPerMessage = 3;\n\t\ttokensPerName = 1;\n\t} else if (model.startsWith("gpt-3.5-turbo")) {\n\t\ttokensPerMessage = 4; // every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n\t\ttokensPerName = -1; // if there\'s a name, the role is omitted\n\t} else {\n\t\tthrow new IllegalArgumentException("Unsupported model: " + model);\n\t}\n\n\tint sum = 0;\n\tfor (final var message : messages) {\n\t\tsum += tokensPerMessage;\n\t\tsum += encoding.countTokens(message.getContent());\n\t\tsum += encoding.countTokens(message.getRole());\n\t\tif (message.hasName()) {\n\t\t\tsum += encoding.countTokens(message.getName());\n\t\t\tsum += tokensPerName;\n\t\t}\n\t}\n\n\tsum += 3; // every reply is primed with <|start|>assistant<|message|>\n\n\treturn sum;\n}\n'})})]})}function g(t={}){const{wrapper:e}={...(0,i.R)(),...t.components};return e?(0,o.jsx)(e,{...t,children:(0,o.jsx)(m,{...t})}):m(t)}},8453:(t,e,n)=>{n.d(e,{R:()=>a,x:()=>r});var s=n(6540);const o={},i=s.createContext(o);function a(t){const e=s.useContext(i);return s.useMemo(function(){return"function"==typeof t?t(e):{...e,...t}},[e,t])}function r(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(o):t.components||o:a(t.components),s.createElement(i.Provider,{value:e},t.children)}}}]);