"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[572],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},s=Object.keys(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,s=e.originalType,c=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=l(n),m=o,g=d["".concat(c,".").concat(m)]||d[m]||u[m]||s;return n?r.createElement(g,a(a({ref:t},p),{},{components:n})):r.createElement(g,a({ref:t},p))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var s=n.length,a=new Array(s);a[0]=m;var i={};for(var c in t)hasOwnProperty.call(t,c)&&(i[c]=t[c]);i.originalType=e,i[d]="string"==typeof e?e:o,a[1]=i;for(var l=2;l<s;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},5065:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var r=n(7462),o=(n(7294),n(3905));const s={title:"Counting Tokens for ChatML",sidebar_position:1},a=void 0,i={unversionedId:"getting-started/recipes/chatml",id:"getting-started/recipes/chatml",title:"Counting Tokens for ChatML",description:"If you are using the OpenAI chat models, you need to account for additional tokens that are added to the input text. This recipe shows how to do that. It is based on this OpenAI Cookbook example.",source:"@site/docs/getting-started/recipes/chatml.md",sourceDirName:"getting-started/recipes",slug:"/getting-started/recipes/chatml",permalink:"/docs/getting-started/recipes/chatml",draft:!1,editUrl:"https://github.com/knuddelsgmbh/jtokkit/docs/docs/getting-started/recipes/chatml.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Counting Tokens for ChatML",sidebar_position:1},sidebar:"gettingStarted",previous:{title:"Extending JTokkit",permalink:"/docs/getting-started/extending"}},c={},l=[],p={toc:l},d="wrapper";function u(e){let{components:t,...n}=e;return(0,o.kt)(d,(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"If you are using the OpenAI chat models, you need to account for additional tokens that are added to the input text. This recipe shows how to do that. It is based on this ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"},"OpenAI Cookbook example"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-java"},'private int countMessageTokens(\n        EncodingRegistry registry,\n        String model,\n        List<ChatMessage> messages // consists of role, content and an optional name\n) {\n    Encoding encoding = registry.getEncodingForModel(model).orElseThrow();\n    int tokensPerMessage;\n    int tokensPerName\n    if (model.startsWith("gpt-4")) {\n        tokensPerMessage = 3;\n        tokensPerName = 1;\n    } else if (model.startsWith("gpt-3.5-turbo")) {\n        tokensPerMessage = 4; // every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n        tokensPerName = -1; // if there\'s a name, the role is omitted\n    } else {\n        throw new IllegalArgumentException("Unsupported model: " + model);\n    }\n\n    int sum = 0;\n    for (final var message : messages) {\n        sum += tokensPerMessage;\n        sum += encoding.countTokens(message.getContent());\n        sum += encoding.countTokens(message.getRole());\n        if (message.hasName()) {\n            sum += encoding.countTokens(message.getName());\n            sum += tokensPerName;\n        }\n    }\n\n    sum += 3; // every reply is primed with <|start|>assistant<|message|>\n\n    return sum;\n}\n')))}u.isMDXComponent=!0}}]);